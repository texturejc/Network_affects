{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immersion Week Term 2: What is the Network Structure of the LIS MASc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![poster](network_affects.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bridge](bridges.png)\n",
    "\n",
    "Network theory emerged in the 18th century, when Leonhard Euler solved the famous \"Seven Bridges of Königsberg\" problem by inventing graph theory, reducing the city's layout to a set of nodes (land masses) and edges (bridges). For nearly 200 years, networks were studied primarily through this mathematical lens, focusing on regular, lattice-like structures. The field radically transformed in the mid-20th century with pioneers like Paul Erdős and Alfréd Rényi, who introduced random graph theory to model complex systems, and Stanley Milgram, whose \"small-world\" experiments revealed the social \"six degrees of separation.\" However, the true revolution arrived in the late 1990s with digital computing. Researchers like Duncan Watts, Steven Strogatz, and Albert-László Barabási analyzed massive, real-world datasets—from the World Wide Web to metabolic pathways—and discovered that most natural networks are neither completely regular nor purely random, but instead follow universal laws, such as the scale-free structure where a few \"hub\" nodes hold the system together, fundamentally changing how we view the interconnected world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters ---\n",
    "N = 12  # number of peripheral nodes (adjust as you like)\n",
    "\n",
    "# --- 1. Build graphs ---\n",
    "# Panopticon: star graph (one hub connected to all others, no other edges)\n",
    "G_star = nx.star_graph(N)\n",
    "\n",
    "# Fully connected: complete graph (every node connected to every other)\n",
    "G_complete = nx.complete_graph(N + 1)  # same node count for fair comparison\n",
    "\n",
    "# --- 2. Layout: circular with hub in centre ---\n",
    "def circular_layout_with_center(n_peripheral):\n",
    "    \"\"\"Place node 0 at centre, rest on a circle.\"\"\"\n",
    "    pos = {0: (0.0, 0.0)}\n",
    "    for i in range(1, n_peripheral + 1):\n",
    "        angle = 2 * np.pi * (i - 1) / n_peripheral\n",
    "        pos[i] = (np.cos(angle), np.sin(angle))\n",
    "    return pos\n",
    "\n",
    "pos = circular_layout_with_center(N)\n",
    "\n",
    "# --- 3. Helper to build Plotly traces from a graph + positions ---\n",
    "def make_edge_traces(G, pos, color=\"rgba(150,150,150,0.4)\", width=1.2):\n",
    "    edge_x, edge_y = [], []\n",
    "    for u, v in G.edges():\n",
    "        x0, y0 = pos[u]\n",
    "        x1, y1 = pos[v]\n",
    "        edge_x += [x0, x1, None]\n",
    "        edge_y += [y0, y1, None]\n",
    "    return go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        mode=\"lines\",\n",
    "        line=dict(width=width, color=color),\n",
    "        hoverinfo=\"none\",\n",
    "    )\n",
    "\n",
    "def make_node_trace(G, pos, hub_color=\"#E63946\", node_color=\"#457B9D\", size=18):\n",
    "    node_x = [pos[n][0] for n in G.nodes()]\n",
    "    node_y = [pos[n][1] for n in G.nodes()]\n",
    "    colors = [hub_color if n == 0 else node_color for n in G.nodes()]\n",
    "    sizes  = [size * 1.6 if n == 0 else size for n in G.nodes()]\n",
    "    return go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=sizes, color=colors,\n",
    "                    line=dict(width=1.5, color=\"white\")),\n",
    "        hoverinfo=\"none\",\n",
    "    )\n",
    "\n",
    "# --- 4. Assemble figure with subplots ---\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\n",
    "        \"<b>Panopticon (Star)</b><br><sup>Maximum hierarchy · single point of control</sup>\",\n",
    "        \"<b>Fully Connected</b><br><sup>Zero hierarchy · total equality</sup>\",\n",
    "    ),\n",
    "    horizontal_spacing=0.08,\n",
    ")\n",
    "\n",
    "# Panopticon (left)\n",
    "fig.add_trace(make_edge_traces(G_star, pos, color=\"rgba(69,123,157,0.5)\", width=2), row=1, col=1)\n",
    "fig.add_trace(make_node_trace(G_star, pos), row=1, col=1)\n",
    "\n",
    "# Fully connected (right) – use a different accent to visually distinguish\n",
    "fig.add_trace(make_edge_traces(G_complete, pos, color=\"rgba(42,157,143,0.25)\", width=1), row=1, col=2)\n",
    "fig.add_trace(make_node_trace(G_complete, pos, hub_color=\"#2A9D8F\", node_color=\"#2A9D8F\", size=16), row=1, col=2)\n",
    "\n",
    "# --- 5. Styling ---\n",
    "axis_defaults = dict(\n",
    "    showgrid=False, zeroline=False, showticklabels=False,\n",
    "    showline=False, scaleratio=1,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(**axis_defaults)\n",
    "fig.update_yaxes(**axis_defaults, scaleanchor=\"x\")   # col 1\n",
    "fig.update_yaxes(**axis_defaults, scaleanchor=\"x2\", row=1, col=2)  # col 2\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1000, height=520,\n",
    "    showlegend=False,\n",
    "    margin=dict(l=30, r=30, t=80, b=30),\n",
    "    paper_bgcolor=\"white\",\n",
    "    plot_bgcolor=\"white\",\n",
    "    title=dict(\n",
    "        text=\"<b>Network Topology Comparison: Two Extremes</b>\",\n",
    "        x=0.5, font=dict(size=40),\n",
    "    ),\n",
    "    font=dict(family=\"Arial, sans-serif\"),\n",
    ")\n",
    "\n",
    "# Add edge-count annotations\n",
    "for col, G, label in [(1, G_star, \"star\"), (2, G_complete, \"complete\")]:\n",
    "    n_edges = G.number_of_edges()\n",
    "    fig.add_annotation(\n",
    "        text=f\"{G.number_of_nodes()} nodes · {n_edges} edges\",\n",
    "        xref=f\"x{col if col > 1 else ''} domain\", yref=f\"y{col if col > 1 else ''} domain\",\n",
    "        x=0.5, y=-0.05, showarrow=False,\n",
    "        font=dict(size=11, color=\"gray\"),\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Structure of the LIS MASc Cohort\n",
    "\n",
    "This notebook builds **text-similarity networks** from the MASc cohort survey responses and uses them to introduce core concepts in network theory. Each student becomes a node; edges connect students whose writing is similar.\n",
    "\n",
    "We construct **two networks**:\n",
    "1. **Short-text network** — based on the ~120-word description of intellectual interests and values.\n",
    "2. **Long-text network** — based on the longer reflective writing piece (e.g. Critical Reflection, capstone plan).\n",
    "\n",
    "The notebook is designed to be **re-run as new survey responses arrive** — it reads whatever rows are present in the Excel file.\n",
    "\n",
    "### Concepts covered\n",
    "1. Nodes and edges (adjacency matrices)\n",
    "2. Degree and degree distribution\n",
    "3. Paths and distances\n",
    "4. Clustering coefficient\n",
    "5. Centrality and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Setup & Data Loading ──────────────────────────────────────────────────────\n",
    "\n",
    "!pip install networkx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ── Load data ─────────────────────────────────────────────────────────────────\n",
    "DATA_FILE = \"What's the network structure of the LIS MASc cohort_(1-28).xlsx\"\n",
    "df_raw = pd.read_excel(DATA_FILE)\n",
    "\n",
    "# Column references (by position — resilient to minor name changes)\n",
    "COL_PSEUDO = df_raw.columns[6]   # pseudonym\n",
    "COL_SHORT  = df_raw.columns[7]   # short text (~120 words)\n",
    "COL_LONG   = df_raw.columns[8]   # long reflective text\n",
    "COL_QUESTION = df_raw.columns[9] # fun question\n",
    "\n",
    "# Build a clean working dataframe\n",
    "df = df_raw[[COL_PSEUDO, COL_SHORT, COL_LONG, COL_QUESTION]].copy()\n",
    "df.columns = ['pseudonym', 'short_text', 'long_text', 'question']\n",
    "\n",
    "# Fill missing text with empty strings\n",
    "df['short_text'] = df['short_text'].fillna('').astype(str)\n",
    "df['long_text']  = df['long_text'].fillna('').astype(str)\n",
    "\n",
    "# Flag very short entries\n",
    "df['short_len'] = df['short_text'].str.len()\n",
    "df['long_len']  = df['long_text'].str.len()\n",
    "\n",
    "print(f\"Loaded {len(df)} respondents from {DATA_FILE}\\n\")\n",
    "display(df[['pseudonym', 'short_len', 'long_len']].style\n",
    "        .set_caption('Respondent summary')\n",
    "        .bar(subset=['short_len', 'long_len'], color='#b3d1ff'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing: TF-IDF and Cosine Similarity\n",
    "\n",
    "### TF-IDF (Term Frequency – Inverse Document Frequency)\n",
    "\n",
    "TF-IDF converts raw text into numerical vectors that capture how important each word is to a particular document *relative to the whole corpus*.\n",
    "\n",
    "For a term $t$ in document $d$ within a corpus $D$:\n",
    "\n",
    "$$\\text{tf}(t, d) = \\frac{\\text{count of } t \\text{ in } d}{\\text{total terms in } d}$$\n",
    "\n",
    "$$\\text{idf}(t, D) = \\log \\frac{|D|}{1 + |\\{d \\in D : t \\in d\\}|}$$\n",
    "\n",
    "$$\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)$$\n",
    "\n",
    "Words that appear frequently in one document but rarely across the corpus receive high TF-IDF scores — they are *distinctive* to that document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Toy Example: TF-IDF and Cosine Similarity ────────────────────────────────\n",
    "# Three short documents about different topics.\n",
    "\n",
    "toy_docs = [\n",
    "    \"The cat sat on the mat and the cat purred\",\n",
    "    \"The dog chased the cat across the garden\",\n",
    "    \"Python is a popular programming language for data science\"\n",
    "]\n",
    "toy_labels = [\"Doc A (cat)\", \"Doc B (cat & dog)\", \"Doc C (programming)\"]\n",
    "\n",
    "# Compute TF-IDF\n",
    "toy_vec = TfidfVectorizer()\n",
    "toy_tfidf = toy_vec.fit_transform(toy_docs)\n",
    "\n",
    "# Show the TF-IDF matrix: each row is a document, each column is a word\n",
    "tfidf_df = pd.DataFrame(\n",
    "    toy_tfidf.toarray().round(3),\n",
    "    index=toy_labels,\n",
    "    columns=toy_vec.get_feature_names_out()\n",
    ")\n",
    "print(\"TF-IDF matrix (rows = documents, columns = words):\")\n",
    "display(tfidf_df.style.background_gradient(cmap='YlOrRd', axis=None)\n",
    "        .format('{:.3f}'))\n",
    "\n",
    "# Compute pairwise cosine similarity\n",
    "toy_sim = cosine_similarity(toy_tfidf)\n",
    "sim_df = pd.DataFrame(toy_sim.round(3), index=toy_labels, columns=toy_labels)\n",
    "\n",
    "print(\"\\nCosine similarity between documents:\")\n",
    "display(sim_df.style.background_gradient(cmap='Blues').format('{:.3f}'))\n",
    "\n",
    "print(\"\\nDocs A and B share the word 'cat' → moderate similarity (≈0.3).\")\n",
    "print(\"Doc C shares no words with A or B → similarity ≈ 0.\")\n",
    "print(\"Each document has similarity 1.0 with itself (the diagonal).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Applied: TF-IDF & Cosine Similarity on Student Data ──────────────────────\n",
    "\n",
    "def compute_similarity(texts, names):\n",
    "    \"\"\"Compute TF-IDF and pairwise cosine similarity for a list of texts.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', min_df=1)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    np.fill_diagonal(sim_matrix, 0)  # zero the diagonal — no self-loops in our network\n",
    "    return pd.DataFrame(sim_matrix, index=names, columns=names)\n",
    "\n",
    "names = df['pseudonym'].tolist()\n",
    "\n",
    "sim_short = compute_similarity(df['short_text'], names)\n",
    "sim_long  = compute_similarity(df['long_text'], names)\n",
    "\n",
    "print(\"Short-text similarity matrix:\")\n",
    "print(\"(Diagonal is zeroed out — a student's similarity with themselves is always\")\n",
    "print(\" 1.0, but self-loops are meaningless in our network, so we set it to 0.)\\n\")\n",
    "display(sim_short.style.background_gradient(cmap='Blues', vmin=0, vmax=0.5)\n",
    "        .format('{:.3f}').set_caption('Cosine similarity — short text'))\n",
    "\n",
    "print(\"\\nLong-text similarity matrix:\")\n",
    "display(sim_long.style.background_gradient(cmap='Oranges', vmin=0, vmax=0.5)\n",
    "        .format('{:.3f}').set_caption('Cosine similarity — long text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Construction\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "A **weighted undirected graph** is a triple $G = (V, E, w)$ where:\n",
    "- $V$ is the set of **nodes** (here: students),\n",
    "- $E \\subseteq \\binom{V}{2}$ is the set of **edges** (pairs of students),\n",
    "- $w: E \\to \\mathbb{R}_{>0}$ assigns a positive **weight** to each edge (here: cosine similarity).\n",
    "\n",
    "We apply a **threshold** $\\tau$: an edge $(i, j)$ exists only if $\\text{sim}(i, j) \\geq \\tau$. Adjusting $\\tau$ controls how connected the network is — low thresholds create dense networks (many weak connections), high thresholds create sparse ones (only strong connections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Toy Example: Threshold and Graph Construction ─────────────────────────────\n",
    "# We'll use the 3-document similarity matrix from above to build tiny graphs\n",
    "# at different thresholds.\n",
    "\n",
    "toy_sim_matrix = cosine_similarity(toy_tfidf)\n",
    "np.fill_diagonal(toy_sim_matrix, 0)\n",
    "toy_sim_df = pd.DataFrame(toy_sim_matrix, index=toy_labels, columns=toy_labels)\n",
    "\n",
    "print(\"Similarity matrix (diagonal zeroed):\")\n",
    "display(toy_sim_df.round(3))\n",
    "\n",
    "for tau in [0.0, 0.15, 0.40]:\n",
    "    G_toy = nx.Graph()\n",
    "    G_toy.add_nodes_from(toy_labels)\n",
    "    for i, n1 in enumerate(toy_labels):\n",
    "        for j, n2 in enumerate(toy_labels):\n",
    "            if j > i and toy_sim_df.iloc[i, j] >= tau:\n",
    "                G_toy.add_edge(n1, n2, weight=round(toy_sim_df.iloc[i, j], 3))\n",
    "    edges_str = ', '.join(f\"{u}–{v} ({d['weight']:.2f})\" for u, v, d in G_toy.edges(data=True))\n",
    "    print(f\"\\n  τ = {tau:.2f} → {G_toy.number_of_edges()} edge(s): {edges_str or '(none)'}\")\n",
    "\n",
    "print(\"\\nAt τ=0.0 Docs A & B are connected (they share 'cat').\")\n",
    "print(\"At τ=0.40 no edges survive — no pair is similar enough.\")\n",
    "print(\"The threshold acts as a dial between a dense and a sparse network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Helper functions (used throughout the notebook) ───────────────────────────\n",
    "\n",
    "def build_graph(sim_df, threshold):\n",
    "    \"\"\"Build a weighted NetworkX graph from a similarity matrix at a given threshold.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(sim_df.index)\n",
    "    for i, n1 in enumerate(sim_df.index):\n",
    "        for j, n2 in enumerate(sim_df.columns):\n",
    "            if j > i and sim_df.iloc[i, j] >= threshold:\n",
    "                G.add_edge(n1, n2, weight=sim_df.iloc[i, j])\n",
    "    return G\n",
    "\n",
    "\n",
    "def plot_network(G, title, color='#4a90d9'):\n",
    "    \"\"\"Create a Plotly figure of the network using a spring layout.\"\"\"\n",
    "    pos = nx.spring_layout(G, seed=42, k=2/np.sqrt(max(len(G), 1)))\n",
    "\n",
    "    # Edges\n",
    "    edge_traces = []\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        x0, y0 = pos[u]\n",
    "        x1, y1 = pos[v]\n",
    "        weight = d.get('weight', 0)\n",
    "        edge_traces.append(go.Scatter(\n",
    "            x=[x0, x1, None], y=[y0, y1, None],\n",
    "            mode='lines',\n",
    "            line=dict(width=max(0.5, weight * 6), color='rgba(150,150,150,0.5)'),\n",
    "            hoverinfo='text',\n",
    "            text=f\"{u} ↔ {v}: {weight:.3f}\",\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    # Nodes\n",
    "    node_x = [pos[n][0] for n in G.nodes()]\n",
    "    node_y = [pos[n][1] for n in G.nodes()]\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers+text',\n",
    "        text=list(G.nodes()),\n",
    "        textposition='top center',\n",
    "        textfont=dict(size=10),\n",
    "        marker=dict(\n",
    "            size=[max(15, 8 + d * 3) for d in degrees],\n",
    "            color=degrees,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title='Degree'),\n",
    "            line=dict(width=1, color='white')\n",
    "        ),\n",
    "        hoverinfo='text',\n",
    "        hovertext=[f\"{n}\\nDegree: {G.degree(n)}\" for n in G.nodes()],\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=edge_traces + [node_trace])\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        showlegend=False,\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        width=750, height=550,\n",
    "        margin=dict(l=20, r=20, t=50, b=20),\n",
    "        plot_bgcolor='white'\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def build_and_plot(sim_df, threshold, label, color='#4a90d9'):\n",
    "    \"\"\"Build graph at threshold and plot it. Returns the graph.\"\"\"\n",
    "    G = build_graph(sim_df, threshold)\n",
    "    fig = plot_network(G, f\"{label} (threshold τ = {threshold:.2f})\", color)\n",
    "    fig.show()\n",
    "    print(f\"  Nodes: {G.number_of_nodes()}  |  Edges: {G.number_of_edges()}\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Applied: Student Networks ─────────────────────────────────────────────────\n",
    "# Sensible defaults: short texts share more common vocabulary → higher threshold\n",
    "THRESH_SHORT = 0.06\n",
    "THRESH_LONG  = 0.06\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SHORT-TEXT NETWORK\")\n",
    "print(\"=\" * 60)\n",
    "G_short = build_and_plot(sim_short, THRESH_SHORT, 'Short-text network')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LONG-TEXT NETWORK\")\n",
    "print(\"=\" * 60)\n",
    "G_long = build_and_plot(sim_long, THRESH_LONG, 'Long-text network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Threshold Explorer ────────────────────────────────────────────────────────\n",
    "# Use this cell to experiment with different thresholds.\n",
    "# Change the values below and re-run to see how the network changes.\n",
    "\n",
    "EXPLORE_THRESHOLD = 0.1   # ← change this value\n",
    "EXPLORE_WHICH = 'short'    # ← 'short' or 'long'\n",
    "\n",
    "sim = sim_short if EXPLORE_WHICH == 'short' else sim_long\n",
    "label = 'Short-text' if EXPLORE_WHICH == 'short' else 'Long-text'\n",
    "G_explore = build_and_plot(sim, EXPLORE_THRESHOLD, f'{label} network (explorer)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Nodes and Edges\n",
    "\n",
    "### Formal Definitions\n",
    "\n",
    "- A **node** (or vertex) $v \\in V$ represents an entity — here, a student.\n",
    "- An **edge** $e = \\{u, v\\} \\in E$ represents a relationship — here, textual similarity above the threshold.\n",
    "- The **adjacency matrix** $A$ of a graph with $n$ nodes is an $n \\times n$ matrix where:\n",
    "\n",
    "$$A_{ij} = \\begin{cases} w(i,j) & \\text{if } \\{i,j\\} \\in E \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "For unweighted graphs, $A_{ij} \\in \\{0, 1\\}$. For weighted graphs (ours), $A_{ij}$ stores the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Toy Example: Nodes, Edges, and Adjacency Matrix ──────────────────────────\n",
    "# A small network of 5 people at a party. Edges represent who talked to whom.\n",
    "\n",
    "G_party = nx.Graph()\n",
    "G_party.add_nodes_from(['Alice', 'Bob', 'Carol', 'Dan', 'Eve'])\n",
    "G_party.add_edges_from([\n",
    "    ('Alice', 'Bob'),    # Alice and Bob chatted\n",
    "    ('Alice', 'Carol'),  # Alice and Carol chatted\n",
    "    ('Bob', 'Carol'),    # Bob and Carol chatted\n",
    "    ('Carol', 'Dan'),    # Carol and Dan chatted\n",
    "    # Eve talked to nobody — she's an isolated node\n",
    "])\n",
    "\n",
    "print(\"Party network:\")\n",
    "print(f\"  Nodes (V): {list(G_party.nodes())}\")\n",
    "print(f\"  Edges (E): {list(G_party.edges())}\")\n",
    "print(f\"  |V| = {G_party.number_of_nodes()}, |E| = {G_party.number_of_edges()}\")\n",
    "\n",
    "# Adjacency matrix\n",
    "party_nodes = ['Alice', 'Bob', 'Carol', 'Dan', 'Eve']\n",
    "adj = nx.to_pandas_adjacency(G_party, nodelist=party_nodes, dtype=int)\n",
    "print(\"\\nAdjacency matrix (unweighted — 1 means connected, 0 means not):\")\n",
    "display(adj.style.background_gradient(cmap='Blues', vmin=0, vmax=1))\n",
    "\n",
    "print(\"\\nNotice: the matrix is symmetric (Alice→Bob = Bob→Alice) and the\")\n",
    "print(\"diagonal is 0 (no self-loops). Eve's row and column are all zeros — she's isolated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Applied: Nodes & Edges in Student Networks ───────────────────────────────\n",
    "\n",
    "def show_adjacency_and_edges(G, label, cmap='Blues'):\n",
    "    \"\"\"Display adjacency matrix heatmap and edge list for a graph.\"\"\"\n",
    "    nodes = list(G.nodes())\n",
    "    adj = nx.to_pandas_adjacency(G, nodelist=nodes)\n",
    "\n",
    "    # Adjacency heatmap\n",
    "    fig = px.imshow(\n",
    "        adj.values, x=nodes, y=nodes,\n",
    "        color_continuous_scale=cmap,\n",
    "        title=f'{label} — Adjacency Matrix (weighted)',\n",
    "        labels=dict(color='Similarity')\n",
    "    )\n",
    "    fig.update_layout(width=650, height=550)\n",
    "    fig.show()\n",
    "\n",
    "    # Edge list\n",
    "    edges = [(u, v, f\"{d['weight']:.3f}\") for u, v, d in\n",
    "             sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)]\n",
    "    if edges:\n",
    "        print(f\"\\n{label} — Edge list ({len(edges)} edges), sorted by weight:\")\n",
    "        edge_df = pd.DataFrame(edges, columns=['Student A', 'Student B', 'Similarity'])\n",
    "        display(edge_df)\n",
    "    else:\n",
    "        print(f\"\\n{label} — No edges at this threshold.\")\n",
    "\n",
    "    print(f\"\\n  Nodes: {G.number_of_nodes()}  |  Edges: {G.number_of_edges()}\")\n",
    "\n",
    "\n",
    "print(\"SHORT-TEXT NETWORK\")\n",
    "show_adjacency_and_edges(G_short, 'Short text', 'Blues')\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nLONG-TEXT NETWORK\")\n",
    "show_adjacency_and_edges(G_long, 'Long text', 'Oranges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Degree and Degree Distribution\n",
    "\n",
    "### Formal Definitions\n",
    "\n",
    "- The **degree** of a node $v$ is the number of edges incident to it: $\\deg(v) = |\\{u : \\{u,v\\} \\in E\\}|$.\n",
    "- The **weighted degree** (or **strength**) of a node is the sum of its edge weights: $s(v) = \\sum_{u: \\{u,v\\} \\in E} w(u,v)$.\n",
    "- The **degree distribution** $P(k)$ gives the fraction of nodes with degree $k$.\n",
    "\n",
    "In a random graph (Erdos–Renyi model $G(n, p)$), the degree distribution follows a binomial distribution centred at $(n-1)p$. Deviations from this tell us the network has non-random structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Toy Example: Degree ──────────────────────────────────────────────────────\n",
    "# Using our party network from above.\n",
    "\n",
    "print(\"Party network degrees:\")\n",
    "for person in party_nodes:\n",
    "    nbrs = list(G_party.neighbors(person))\n",
    "    print(f\"  {person:6s}  deg = {G_party.degree(person)}  \"\n",
    "          f\"(neighbours: {', '.join(nbrs) if nbrs else 'none'})\")\n",
    "\n",
    "print(f\"\\nMean degree: {np.mean([G_party.degree(n) for n in G_party.nodes()]):.1f}\")\n",
    "print(\"\\nCarol has the highest degree (3) — she talked to the most people.\")\n",
    "print(\"Eve has degree 0 — she's isolated.\")\n",
    "print(\"\\nIn a weighted network, 'strength' sums the edge weights instead of\")\n",
    "print(\"just counting edges — it distinguishes strong from weak connections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Applied: Degree & Degree Distribution in Student Networks ─────────────────\n",
    "\n",
    "def degree_analysis(G, label, color):\n",
    "    nodes = list(G.nodes())\n",
    "    degrees = {n: G.degree(n) for n in nodes}\n",
    "    strengths = {n: round(G.degree(n, weight='weight'), 3) for n in nodes}\n",
    "\n",
    "    deg_df = pd.DataFrame({\n",
    "        'Degree': degrees,\n",
    "        'Strength (weighted degree)': strengths\n",
    "    }).sort_values('Degree', ascending=False)\n",
    "\n",
    "    # Bar chart of degree per student\n",
    "    fig = px.bar(\n",
    "        deg_df.reset_index(), x='index', y='Degree',\n",
    "        title=f'{label} — Degree per student',\n",
    "        labels={'index': 'Student', 'Degree': 'Degree'},\n",
    "        color='Degree', color_continuous_scale='Viridis'\n",
    "    )\n",
    "    fig.update_layout(width=700, height=400)\n",
    "    fig.show()\n",
    "\n",
    "    # Degree distribution\n",
    "    deg_values = list(degrees.values())\n",
    "    fig2 = px.histogram(\n",
    "        x=deg_values, nbins=max(deg_values) - min(deg_values) + 1 if deg_values else 1,\n",
    "        title=f'{label} — Degree distribution',\n",
    "        labels={'x': 'Degree', 'y': 'Count'},\n",
    "        color_discrete_sequence=[color]\n",
    "    )\n",
    "    fig2.update_layout(width=600, height=350, bargap=0.1)\n",
    "    fig2.show()\n",
    "\n",
    "    # Erdos-Renyi comparison\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    max_edges = n * (n - 1) / 2\n",
    "    p = m / max_edges if max_edges > 0 else 0\n",
    "    expected_deg = (n - 1) * p\n",
    "    print(f\"  Edge density: {p:.3f}\")\n",
    "    print(f\"  Expected degree in Erdos–Renyi G({n}, {p:.3f}): {expected_deg:.1f}\")\n",
    "    print(f\"  Actual mean degree: {np.mean(deg_values):.1f}\")\n",
    "    print(f\"  Actual std of degree: {np.std(deg_values):.2f}  \"\n",
    "          f\"(Erdos–Renyi std ≈ {np.sqrt((n-1)*p*(1-p)):.2f})\")\n",
    "\n",
    "    display(deg_df)\n",
    "    return deg_df\n",
    "\n",
    "\n",
    "print(\"SHORT-TEXT NETWORK\")\n",
    "deg_short = degree_analysis(G_short, 'Short text', '#4a90d9')\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nLONG-TEXT NETWORK\")\n",
    "deg_long = degree_analysis(G_long, 'Long text', '#d97a4a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Paths and Distances\n",
    "\n",
    "### Formal Definitions\n",
    "\n",
    "- A **path** from $u$ to $v$ is a sequence of edges connecting them: $u = v_0, v_1, \\ldots, v_k = v$ where each $\\{v_{i}, v_{i+1}\\} \\in E$.\n",
    "- The **shortest path length** (or geodesic distance) $d(u, v)$ is the minimum number of edges in any path from $u$ to $v$.\n",
    "- The **diameter** of a connected graph is the longest shortest path: $\\text{diam}(G) = \\max_{u,v \\in V} d(u,v)$.\n",
    "- The **average path length** is $\\bar{L} = \\frac{1}{n(n-1)} \\sum_{u \\neq v} d(u,v)$.\n",
    "- A **connected component** is a maximal subgraph in which every pair of nodes is connected by a path. If $\\tau$ is high enough, the network may fragment into separate components — students with no path between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Toy Example: Paths and Distances ─────────────────────────────────────────\n",
    "# A slightly larger network: two triangles connected by a single bridge.\n",
    "#\n",
    "#   A ── B         E ── F\n",
    "#    \\ /           \\ /\n",
    "#     C ── bridge ── D\n",
    "\n",
    "G_bridge = nx.Graph()\n",
    "G_bridge.add_edges_from([\n",
    "    ('A', 'B'), ('A', 'C'), ('B', 'C'),   # left triangle\n",
    "    ('C', 'D'),                             # bridge\n",
    "    ('D', 'E'), ('D', 'F'), ('E', 'F'),   # right triangle\n",
    "])\n",
    "\n",
    "# Shortest path from A to F\n",
    "path_AF = nx.shortest_path(G_bridge, 'A', 'F')\n",
    "print(f\"Shortest path A → F: {' → '.join(path_AF)}  (length {len(path_AF)-1})\")\n",
    "\n",
    "path_AF2 = nx.shortest_path(G_bridge, 'A', 'E')\n",
    "print(f\"Shortest path A → E: {' → '.join(path_AF2)}  (length {len(path_AF2)-1})\")\n",
    "\n",
    "path_AB = nx.shortest_path(G_bridge, 'A', 'B')\n",
    "print(f\"Shortest path A → B: {' → '.join(path_AB)}  (length {len(path_AB)-1})\")\n",
    "\n",
    "print(f\"\\nDiameter (longest shortest path): {nx.diameter(G_bridge)}\")\n",
    "print(f\"Average path length: {nx.average_shortest_path_length(G_bridge):.2f}\")\n",
    "\n",
    "# Connected components\n",
    "print(f\"Connected components: {nx.number_connected_components(G_bridge)}\")\n",
    "\n",
    "# Now remove the bridge and see what happens\n",
    "G_broken = G_bridge.copy()\n",
    "G_broken.remove_edge('C', 'D')\n",
    "print(f\"\\nAfter removing the C–D bridge:\")\n",
    "print(f\"  Connected components: {nx.number_connected_components(G_broken)}\")\n",
    "for i, comp in enumerate(nx.connected_components(G_broken)):\n",
    "    print(f\"  Component {i+1}: {sorted(comp)}\")\n",
    "print(\"  A and F are now unreachable from each other — no path exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Applied: Paths & Distances in Student Networks ────────────────────────────\n",
    "\n",
    "def path_analysis(G, label, cmap='Blues'):\n",
    "    nodes = list(G.nodes())\n",
    "    n = len(nodes)\n",
    "\n",
    "    # Connected components\n",
    "    components = list(nx.connected_components(G))\n",
    "    print(f\"  Connected components: {len(components)}\")\n",
    "    for i, comp in enumerate(sorted(components, key=len, reverse=True)):\n",
    "        print(f\"    Component {i+1} ({len(comp)} nodes): {sorted(comp)}\")\n",
    "\n",
    "    # Shortest path length matrix\n",
    "    sp_matrix = np.full((n, n), np.inf)\n",
    "    np.fill_diagonal(sp_matrix, 0)\n",
    "    path_lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    for i, n1 in enumerate(nodes):\n",
    "        for j, n2 in enumerate(nodes):\n",
    "            if n2 in path_lengths.get(n1, {}):\n",
    "                sp_matrix[i, j] = path_lengths[n1][n2]\n",
    "\n",
    "    # Replace inf with NaN for display\n",
    "    sp_display = np.where(np.isinf(sp_matrix), np.nan, sp_matrix)\n",
    "\n",
    "    fig = px.imshow(\n",
    "        sp_display, x=nodes, y=nodes,\n",
    "        color_continuous_scale=cmap,\n",
    "        title=f'{label} — Shortest path lengths',\n",
    "        labels=dict(color='Path length')\n",
    "    )\n",
    "    fig.update_layout(width=900, height=700)\n",
    "    fig.show()\n",
    "\n",
    "    # Diameter and average path length (per component)\n",
    "    for i, comp in enumerate(sorted(components, key=len, reverse=True)):\n",
    "        if len(comp) > 1:\n",
    "            subG = G.subgraph(comp)\n",
    "            diam = nx.diameter(subG)\n",
    "            avg_pl = nx.average_shortest_path_length(subG)\n",
    "            print(f\"\\n  Component {i+1}: diameter = {diam}, average path length = {avg_pl:.2f}\")\n",
    "\n",
    "    # Most/least reachable pairs\n",
    "    finite_mask = np.isfinite(sp_matrix) & (sp_matrix > 0)\n",
    "    if finite_mask.any():\n",
    "        max_idx = np.unravel_index(np.where(finite_mask, sp_matrix, 0).argmax(), sp_matrix.shape)\n",
    "        print(f\"\\n  Most distant reachable pair: {nodes[max_idx[0]]} ↔ {nodes[max_idx[1]]} \"\n",
    "              f\"(path length {int(sp_matrix[max_idx])})\")\n",
    "\n",
    "    # Unreachable pairs\n",
    "    unreachable = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if np.isinf(sp_matrix[i, j]):\n",
    "                unreachable.append((nodes[i], nodes[j]))\n",
    "    if unreachable:\n",
    "        print(f\"\\n  Unreachable pairs ({len(unreachable)}):\")\n",
    "        for a, b in unreachable[:10]:\n",
    "            print(f\"    {a} ↔ {b}\")\n",
    "        if len(unreachable) > 10:\n",
    "            print(f\"    ... and {len(unreachable) - 10} more\")\n",
    "\n",
    "\n",
    "print(\"SHORT-TEXT NETWORK\")\n",
    "path_analysis(G_short, 'Short text', 'Blues')\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nLONG-TEXT NETWORK\")\n",
    "path_analysis(G_long, 'Long text', 'Oranges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Clustering Coefficient\n",
    "\n",
    "### Formal Definitions\n",
    "\n",
    "- The **local clustering coefficient** of node $v$ measures how connected its neighbours are to each other:\n",
    "\n",
    "$$C(v) = \\frac{2 |\\{\\{u,w\\} \\in E : u,w \\in N(v)\\}|}{\\deg(v)(\\deg(v) - 1)}$$\n",
    "\n",
    "where $N(v)$ is the set of neighbours of $v$. If $\\deg(v) < 2$, we define $C(v) = 0$.\n",
    "\n",
    "- The **global (average) clustering coefficient** is $\\bar{C} = \\frac{1}{n} \\sum_{v \\in V} C(v)$.\n",
    "- **Transitivity** is the ratio of closed triplets to all triplets: $T = \\frac{3 \\times \\text{triangles}}{\\text{connected triples}}$.\n",
    "\n",
    "High clustering means that \"friends of friends tend to be friends\" — or in our context, that students with similar writing tend to form tight clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Toy Example: Clustering Coefficient ──────────────────────────────────────\n",
    "# Using our bridge graph:  A─B─C (triangle) ── D─E─F (triangle)\n",
    "#\n",
    "# C has 3 neighbours: A, B, D.\n",
    "#   Of the 3 possible edges among {A, B, D}, only A–B exists.\n",
    "#   So C(C) = 2·1 / (3·2) = 1/3 ≈ 0.333\n",
    "#\n",
    "# A has 2 neighbours: B, C.\n",
    "#   The only possible edge among {B, C} is B–C, and it exists.\n",
    "#   So C(A) = 2·1 / (2·1) = 1.0\n",
    "\n",
    "print(\"Bridge graph — local clustering coefficients:\")\n",
    "print()\n",
    "for node in sorted(G_bridge.nodes()):\n",
    "    nbrs = sorted(G_bridge.neighbors(node))\n",
    "    cc = nx.clustering(G_bridge, node)\n",
    "    print(f\"  {node}: C = {cc:.3f}  (neighbours: {', '.join(nbrs)})\")\n",
    "\n",
    "print(f\"\\nWorked example for C:\")\n",
    "print(f\"  Neighbours of C: A, B, D\")\n",
    "print(f\"  Possible edges among them: A–B, A–D, B–D  (3 possible)\")\n",
    "print(f\"  Actual edges among them: A–B only  (1 actual)\")\n",
    "print(f\"  C(C) = 2×1 / (3×2) = 0.333\")\n",
    "print(f\"\\nWorked example for A:\")\n",
    "print(f\"  Neighbours of A: B, C\")\n",
    "print(f\"  Possible edges among them: B–C  (1 possible)\")\n",
    "print(f\"  Actual edges among them: B–C  (1 actual)\")\n",
    "print(f\"  C(A) = 2×1 / (2×1) = 1.000\")\n",
    "print(f\"\\nA sits inside a tight triangle → C = 1.\")\n",
    "print(f\"C and D bridge two clusters → lower C (not all their friends know each other).\")\n",
    "print(f\"\\nAverage clustering: {nx.average_clustering(G_bridge):.3f}\")\n",
    "print(f\"Transitivity: {nx.transitivity(G_bridge):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Applied: Clustering Coefficient in Student Networks ───────────────────────\n",
    "\n",
    "def clustering_analysis(G, label, color):\n",
    "    cc = nx.clustering(G)\n",
    "    cc_df = pd.DataFrame.from_dict(cc, orient='index', columns=['Clustering coeff.'])\n",
    "    cc_df = cc_df.sort_values('Clustering coeff.', ascending=False)\n",
    "\n",
    "    fig = px.bar(\n",
    "        cc_df.reset_index(), x='index', y='Clustering coeff.',\n",
    "        title=f'{label} — Local clustering coefficient per student',\n",
    "        labels={'index': 'Student'},\n",
    "        color='Clustering coeff.', color_continuous_scale='Viridis'\n",
    "    )\n",
    "    fig.update_layout(width=700, height=400)\n",
    "    fig.show()\n",
    "\n",
    "    avg_cc = nx.average_clustering(G)\n",
    "    trans = nx.transitivity(G)\n",
    "    print(f\"  Average clustering coefficient: {avg_cc:.3f}\")\n",
    "    print(f\"  Transitivity: {trans:.3f}\")\n",
    "\n",
    "    # Interpretation\n",
    "    high_cc = cc_df[cc_df['Clustering coeff.'] > 0.5]\n",
    "    if not high_cc.empty:\n",
    "        print(f\"\\n  Students in tight clusters (C > 0.5): {', '.join(high_cc.index)}\")\n",
    "\n",
    "    bridges = cc_df[(cc_df['Clustering coeff.'] < 0.3) &\n",
    "                    (cc_df['Clustering coeff.'] > 0)]\n",
    "    if not bridges.empty:\n",
    "        print(f\"  Potential bridges (low C, but connected): {', '.join(bridges.index)}\")\n",
    "\n",
    "    display(cc_df)\n",
    "    return cc_df\n",
    "\n",
    "\n",
    "print(\"SHORT-TEXT NETWORK\")\n",
    "cc_short = clustering_analysis(G_short, 'Short text', '#4a90d9')\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nLONG-TEXT NETWORK\")\n",
    "cc_long = clustering_analysis(G_long, 'Long text', '#d97a4a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Centrality and Outliers\n",
    "\n",
    "### Formal Definitions\n",
    "\n",
    "Centrality measures quantify how \"important\" or \"influential\" a node is within the network. Different measures capture different notions of importance:\n",
    "\n",
    "- **Degree centrality**: $C_D(v) = \\frac{\\deg(v)}{n - 1}$ — fraction of possible connections realised. High degree centrality = connected to many others.\n",
    "\n",
    "- **Betweenness centrality**: $C_B(v) = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}$ where $\\sigma_{st}$ is the number of shortest paths from $s$ to $t$, and $\\sigma_{st}(v)$ is the number that pass through $v$. High betweenness = sits on many shortest paths (a \"bridge\" or \"gatekeeper\").\n",
    "\n",
    "- **Closeness centrality**: $C_C(v) = \\frac{n - 1}{\\sum_{u \\neq v} d(v, u)}$ — inverse of average distance to all other nodes. High closeness = can reach everyone quickly.\n",
    "\n",
    "- **Eigenvector centrality**: solves $A\\mathbf{x} = \\lambda \\mathbf{x}$ — a node is important if it is connected to other important nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Toy Example: Centrality ──────────────────────────────────────────────────\n",
    "# Using the bridge graph: A─B─C (triangle) ── D─E─F (triangle)\n",
    "# This graph makes the differences between centrality measures vivid.\n",
    "\n",
    "toy_cent = pd.DataFrame({\n",
    "    'Degree':      nx.degree_centrality(G_bridge),\n",
    "    'Betweenness': nx.betweenness_centrality(G_bridge),\n",
    "    'Closeness':   nx.closeness_centrality(G_bridge),\n",
    "    'Eigenvector':  nx.eigenvector_centrality(G_bridge),\n",
    "}).round(3)\n",
    "\n",
    "print(\"Bridge graph — centrality measures:\")\n",
    "display(toy_cent)\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  • Degree centrality: C and D have the most connections (3 each).\")\n",
    "print(\"  • Betweenness: C and D dominate — every shortest path between the two\")\n",
    "print(\"    triangles must pass through them. They are gatekeepers.\")\n",
    "print(\"  • Closeness: C and D are closest to everyone on average.\")\n",
    "print(\"  • Eigenvector: C and D score highest — they're connected to each other\")\n",
    "print(\"    (both important), while A is only connected to B and C.\")\n",
    "print(\"\\nIn this symmetric graph, all four measures agree. In real networks,\")\n",
    "print(\"they often diverge — a node can be a bridge (high betweenness) without\")\n",
    "print(\"having many connections (low degree).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Applied: Centrality & Outliers in Student Networks ────────────────────────\n",
    "\n",
    "def centrality_analysis(G, sim_df, label, color):\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    # Compute centralities\n",
    "    deg_c = nx.degree_centrality(G)\n",
    "    bet_c = nx.betweenness_centrality(G)\n",
    "    # Closeness: handle disconnected graphs\n",
    "    clo_c = {}\n",
    "    for comp in nx.connected_components(G):\n",
    "        subG = G.subgraph(comp)\n",
    "        if len(comp) > 1:\n",
    "            sub_clo = nx.closeness_centrality(subG)\n",
    "            clo_c.update(sub_clo)\n",
    "        else:\n",
    "            for n in comp:\n",
    "                clo_c[n] = 0.0\n",
    "    # Eigenvector centrality: handle disconnected graphs\n",
    "    eig_c = {}\n",
    "    for comp in nx.connected_components(G):\n",
    "        subG = G.subgraph(comp)\n",
    "        if len(comp) > 1:\n",
    "            try:\n",
    "                sub_eig = nx.eigenvector_centrality(subG, max_iter=1000)\n",
    "            except nx.PowerIterationFailedConvergence:\n",
    "                sub_eig = {n: 0.0 for n in comp}\n",
    "            eig_c.update(sub_eig)\n",
    "        else:\n",
    "            for n in comp:\n",
    "                eig_c[n] = 0.0\n",
    "\n",
    "    cent_df = pd.DataFrame({\n",
    "        'Degree': deg_c,\n",
    "        'Betweenness': bet_c,\n",
    "        'Closeness': clo_c,\n",
    "        'Eigenvector': eig_c\n",
    "    }).round(3)\n",
    "\n",
    "    # Bar chart comparison\n",
    "    fig = make_subplots(rows=2, cols=2,\n",
    "                        subplot_titles=['Degree centrality', 'Betweenness centrality',\n",
    "                                        'Closeness centrality', 'Eigenvector centrality'])\n",
    "    for idx, col in enumerate(cent_df.columns):\n",
    "        row, c = divmod(idx, 2)\n",
    "        sorted_df = cent_df.sort_values(col, ascending=False)\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=sorted_df.index, y=sorted_df[col], name=col,\n",
    "                   marker_color=color, showlegend=False),\n",
    "            row=row+1, col=c+1\n",
    "        )\n",
    "    fig.update_layout(title=f'{label} — Centrality measures', width=800, height=600)\n",
    "    fig.show()\n",
    "\n",
    "    # Network with node size ∝ eigenvector centrality\n",
    "    pos = nx.spring_layout(G, seed=42, k=2/np.sqrt(max(len(G), 1)))\n",
    "    edge_traces = []\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        x0, y0 = pos[u]\n",
    "        x1, y1 = pos[v]\n",
    "        edge_traces.append(go.Scatter(\n",
    "            x=[x0, x1, None], y=[y0, y1, None],\n",
    "            mode='lines',\n",
    "            line=dict(width=1, color='rgba(150,150,150,0.4)'),\n",
    "            hoverinfo='none', showlegend=False\n",
    "        ))\n",
    "\n",
    "    eig_vals = [eig_c.get(n, 0) for n in G.nodes()]\n",
    "    max_eig = max(eig_vals) if max(eig_vals) > 0 else 1\n",
    "    node_sizes = [max(12, 40 * e / max_eig) for e in eig_vals]\n",
    "\n",
    "    node_trace = go.Scatter(\n",
    "        x=[pos[n][0] for n in G.nodes()],\n",
    "        y=[pos[n][1] for n in G.nodes()],\n",
    "        mode='markers+text',\n",
    "        text=list(G.nodes()),\n",
    "        textposition='top center', textfont=dict(size=9),\n",
    "        marker=dict(\n",
    "            size=node_sizes,\n",
    "            color=eig_vals,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title='Eigenvector<br>centrality'),\n",
    "            line=dict(width=1, color='white')\n",
    "        ),\n",
    "        hovertext=[f\"{n}\\nEigenvector: {eig_c.get(n,0):.3f}\\n\"\n",
    "                   f\"Degree: {deg_c[n]:.3f}\\nBetweenness: {bet_c[n]:.3f}\"\n",
    "                   for n in G.nodes()],\n",
    "        hoverinfo='text', showlegend=False\n",
    "    )\n",
    "\n",
    "    fig2 = go.Figure(data=edge_traces + [node_trace])\n",
    "    fig2.update_layout(\n",
    "        title=f'{label} — Node size ∝ eigenvector centrality',\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        width=750, height=550, margin=dict(l=20, r=20, t=50, b=20),\n",
    "        plot_bgcolor='white'\n",
    "    )\n",
    "    fig2.show()\n",
    "\n",
    "    # Identify hubs and outliers\n",
    "    print(f\"\\n  Most central (by eigenvector): \"\n",
    "          f\"{cent_df['Eigenvector'].idxmax()} ({cent_df['Eigenvector'].max():.3f})\")\n",
    "    print(f\"  Most central (by betweenness): \"\n",
    "          f\"{cent_df['Betweenness'].idxmax()} ({cent_df['Betweenness'].max():.3f})\")\n",
    "\n",
    "    isolated = [n for n in G.nodes() if G.degree(n) == 0]\n",
    "    if isolated:\n",
    "        print(f\"  Isolated nodes (degree 0): {', '.join(isolated)}\")\n",
    "\n",
    "    peripheral = cent_df[cent_df['Degree'] <= cent_df['Degree'].quantile(0.25)]\n",
    "    if not peripheral.empty:\n",
    "        print(f\"  Peripheral nodes (bottom 25% degree): {', '.join(peripheral.index)}\")\n",
    "\n",
    "    display(cent_df.sort_values('Eigenvector', ascending=False))\n",
    "    return cent_df\n",
    "\n",
    "\n",
    "print(\"SHORT-TEXT NETWORK\")\n",
    "cent_short = centrality_analysis(G_short, sim_short, 'Short text', '#4a90d9')\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nLONG-TEXT NETWORK\")\n",
    "cent_long = centrality_analysis(G_long, sim_long, 'Long text', '#d97a4a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparison & Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Comparison & Reflection ──────────────────────────────────────────────────\n",
    "\n",
    "# ── Side-by-side summary ─────────────────────────────────────────────────────\n",
    "comparison = pd.DataFrame({\n",
    "    'Short-text network': {\n",
    "        'Nodes': G_short.number_of_nodes(),\n",
    "        'Edges': G_short.number_of_edges(),\n",
    "        'Threshold': THRESH_SHORT,\n",
    "        'Density': f\"{nx.density(G_short):.3f}\",\n",
    "        'Components': nx.number_connected_components(G_short),\n",
    "        'Avg clustering': f\"{nx.average_clustering(G_short):.3f}\",\n",
    "        'Transitivity': f\"{nx.transitivity(G_short):.3f}\",\n",
    "    },\n",
    "    'Long-text network': {\n",
    "        'Nodes': G_long.number_of_nodes(),\n",
    "        'Edges': G_long.number_of_edges(),\n",
    "        'Threshold': THRESH_LONG,\n",
    "        'Density': f\"{nx.density(G_long):.3f}\",\n",
    "        'Components': nx.number_connected_components(G_long),\n",
    "        'Avg clustering': f\"{nx.average_clustering(G_long):.3f}\",\n",
    "        'Transitivity': f\"{nx.transitivity(G_long):.3f}\",\n",
    "    }\n",
    "})\n",
    "print(\"Network Comparison\")\n",
    "display(comparison)\n",
    "\n",
    "# ── Do the same students occupy similar positions? ────────────────────────────\n",
    "print(\"\\n── Centrality correlation across networks ──\")\n",
    "common_nodes = sorted(set(cent_short.index) & set(cent_long.index))\n",
    "for measure in ['Degree', 'Betweenness', 'Closeness', 'Eigenvector']:\n",
    "    s = cent_short.loc[common_nodes, measure]\n",
    "    l = cent_long.loc[common_nodes, measure]\n",
    "    corr = s.corr(l)\n",
    "    print(f\"  {measure:15s} Pearson r = {corr:+.3f}\")\n",
    "\n",
    "print(\"\\nPositive correlations suggest students occupy similar positions in both \")\n",
    "print(\"networks — their short self-description aligns with their longer writing.\")\n",
    "print(\"Low or negative correlations suggest the two texts reveal different facets \")\n",
    "print(\"of intellectual identity.\")\n",
    "\n",
    "# ── Rank comparison ──────────────────────────────────────────────────────────\n",
    "rank_df = pd.DataFrame({\n",
    "    'Short-text eigenvector rank': cent_short.loc[common_nodes, 'Eigenvector']\n",
    "        .rank(ascending=False).astype(int),\n",
    "    'Long-text eigenvector rank': cent_long.loc[common_nodes, 'Eigenvector']\n",
    "        .rank(ascending=False).astype(int),\n",
    "})\n",
    "rank_df['Rank change'] = rank_df.iloc[:, 0] - rank_df.iloc[:, 1]\n",
    "display(rank_df.sort_values('Short-text eigenvector rank'))\n",
    "\n",
    "# ── Fun appendix: student-submitted questions ────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"APPENDIX: Questions submitted by cohort members\")\n",
    "print(\"=\" * 60)\n",
    "for _, row in df.iterrows():\n",
    "    q = row['question']\n",
    "    if pd.notna(q) and str(q).strip() and str(q).strip().lower() not in ('nan', 'answer'):\n",
    "        print(f\"  {row['pseudonym']}: \\\"{q}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
